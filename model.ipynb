{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24940b0",
   "metadata": {},
   "source": [
    "# Projeto Final de Machine Learning\n",
    "Feito por: _Henrique Bucci_ e _Marcelo Alonso_\n",
    "\n",
    "Dados e informações: https://www.kaggle.com/datasets/marcopale/housing/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcca7c3",
   "metadata": {},
   "source": [
    "**Perguntas**  \n",
    "- Posso remover o PID?\n",
    "    - **R:** Sim\n",
    "- Posso criar colunas a partir de contas de outras antes de fazer a seleção?\n",
    "    - **R:** Sim\n",
    "- Se eu aplicar PolynomialFeatures nos dados, eles também contam como features para a contagem?\n",
    "    - **R:** Fazer PolyFeatures depois de selecionar as features\n",
    "- Posso utilizar correlação na análise exploratória?\n",
    "    - **R:** Pode, mas é \"inútil\"\n",
    "- Posso utilizar métodos de clustering na pipeline para incluir a classificação como uma nova feature?\n",
    "    - **R:** SoftMax no resultado do Kmeans para exagerar a classe mais próxima.\n",
    "- Posso utilizar algum método de Dimensionality Reduction (ex: PCA) para me ajudar a escolher as features?\n",
    "    - **R:** Sim.\n",
    "\n",
    "\n",
    "Testar stacking: Treinar diversos modelos e treinar um modelo final com os predicts destes modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f250d",
   "metadata": {},
   "source": [
    "#### ANOTAÇÕES\n",
    "Utilizar LASSO para seleção de features.\n",
    "\n",
    "Regressão linear para ignorar outliers.\n",
    "\n",
    "RANSAC -> regressao linear que ignora outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca61cb26",
   "metadata": {},
   "source": [
    "## Etapa 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0d7266",
   "metadata": {},
   "source": [
    "Nesta etapa, iremos:\n",
    "- Importar bibliotecas\n",
    "- Carregar os dados\n",
    "- Verificar se existem colunas que não fazem sentido serem colocadas no dataset final (como ID ou algum outro tipo de identificador arbitrário), olhando apenas a descrição das colunas.\n",
    "- Separar o dataset em Treino-Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b12dfc8",
   "metadata": {},
   "source": [
    "### Bibliotecas e Configurações Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ae709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing  import FunctionTransformer, StandardScaler, MinMaxScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.cluster        import KMeans\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67d0ee",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c79cf5",
   "metadata": {},
   "source": [
    "### Carregamento e Pré-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f778711",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350570d",
   "metadata": {},
   "source": [
    "Neste caso, a presença de duplicatas não seria intencional, uma vez que cada casa deveria ser única.  \n",
    "Portanto, vamos removê-las."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total de linhas antes de remover duplicatas: {dataset.shape[0]}\")\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "print(f\"Total de linhas depois de remover duplicatas: {dataset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d89e5",
   "metadata": {},
   "source": [
    "Como a primeira coluna é o ID da observação e a segunda é um identificador, podemos removê-las, uma vez que estes são valores arbitrários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.iloc[:, 2:] # Estamos removendo as duas primeiras colunas, que são o ID e o PID (Parcel identification number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91625943",
   "metadata": {},
   "source": [
    "### Criando novas features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05ab06",
   "metadata": {},
   "source": [
    "Ao analisarmos as features da forma descrita acima, vimos espaço para a criação de novas features que podem vir a ser úteis na modelagem dos dados:\n",
    "- **Tot Lot Area** : `Lot Frontage + Lot Area`\n",
    "- **Bsmt Tot Bath** : `Bsmt Full Bath + 0.5*Bsmt Half Bath`\n",
    "- **Garage Area/Car** : `Garage Area / Garage Cars`\n",
    "- **Tot Porch SF** : `Open Porch SF + Enclosed Porch + 3Ssn Porch + Screen Porch`\n",
    "- **Date Sold** : `timestamp(Month Sold, Year Sold)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.loc[:, 'Tot.Lot.Area'] = dataset.loc[:, 'Lot.Frontage'] + dataset.loc[:, 'Lot.Area']\n",
    "dataset.loc[:, 'Bsmt.Tot.Bath'] = dataset.loc[:, 'Bsmt.Full.Bath'] + 0.5*dataset.loc[:, 'Bsmt.Half.Bath']\n",
    "# dataset.loc[:, 'Garage.Area/Cars'] = dataset.loc[:, 'Garage.Area'] / dataset.loc[:, 'Garage.Cars']\n",
    "dataset.loc[:, 'Tot.Porch.SF'] = dataset.loc[:, 'Open.Porch.SF'] + dataset.loc[:, 'X3Ssn.Porch'] + dataset.loc[:, 'Enclosed.Porch'] + dataset.loc[:, 'Screen.Porch']\n",
    "# dataset.loc[:, 'Date.Sold'] = pd.to_datetime(dict(year=dataset['Yr.Sold'], month=dataset['Mo.Sold'], day=1)).apply(lambda x: x.timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59767f97",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762ed9a",
   "metadata": {},
   "source": [
    "- A partir de agora, usaremos apenas o dataset de treino, a partição de teste será tratada como se não existisse ainda.\n",
    "- O dataset total será dividido em uma proporção 80/20, uma vez que temos poucos dados (2930 no total).\n",
    "- Por não se tratar de uma série temporal, podemos aplicar uma aleatoriedade na partição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bafc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset.drop('SalePrice', axis=1), dataset.loc[:, 'SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c82b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafaedf",
   "metadata": {},
   "source": [
    "## Etapa 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299437b7",
   "metadata": {},
   "source": [
    "### Análise Exploratória\n",
    "\n",
    "Nesta parte, iremos fazer uma análise global dos dados, apenas para garantir a integridade destes.  \n",
    "Assim sendo, iremos procurar entender quais são as features e target, quais são seus respectivos tipos e buscar outras informações como:\n",
    "- Dados nulos\n",
    "- Dados duplicados\n",
    "- Outliers\n",
    "- Spikes\n",
    "- Erros grosseiros\n",
    "\n",
    "Além disso, iremos buscar saber a distribuição e a \"cara\" de cada variável."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da621ea8",
   "metadata": {},
   "source": [
    "#### Valores Faltantes e Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c336c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74655dbd",
   "metadata": {},
   "source": [
    "#### Distribuição dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e69a6",
   "metadata": {},
   "source": [
    "Nesta parte, iremos olhar especificamente para a distribuição dos dados.  \n",
    "Nas células abaixo conseguimos ver:\n",
    "- Distribuição dos dados numéricos, com os valores de `count`, `min`, `max`, `std`, `mean`, e os quartis.\n",
    "- Distribuição dos dados categóricos, com os valores de `count`, `unique`, `top` (moda), `freq` (número de ocorrências da moda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6785233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe(include=np.object_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070868a",
   "metadata": {},
   "source": [
    "##### Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f13ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(X_train, 'x_train_original.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944aed12",
   "metadata": {},
   "source": [
    "Para uma visualização melhor fizemos este gráfico, e nele podemos ver que diversas features que são estitamente positivas e possuem uma cauda direita alongada.  \n",
    "  \n",
    "Neste caso, o ideal é transformá-las em distribuições normais.  \n",
    "\n",
    "<img src=\"./graphs/x_train_original.png\" alt=\"drawing\" width=\"700\"/>  \n",
    "  \n",
    "Assim sendo, aplicaremos log nas colunas que possuem uma cauda direita, e iremos fazer um gráfico para visualizarmos as diferenças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pegando os nomes das colunas numéricas, categóricas e com cauda direita alongada \n",
    "para fazermos as transformações necessárias.\n",
    "Estas variáveis serão utilizadas durante todo o notebook.\n",
    "\"\"\"\n",
    "right_skewed, numerical, categorical = get_column_subsets(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_log = X_train.copy()\n",
    "X_train_log[right_skewed] = np.log1p(X_train_log[right_skewed])\n",
    "\n",
    "plot_distribution(X_train_log.select_dtypes(include='number'), 'x_train_log.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b670b",
   "metadata": {},
   "source": [
    "<img src=\"./graphs/x_train_log.png\" alt=\"drawing\" width=\"700\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2072960",
   "metadata": {},
   "source": [
    "#### Distribuição do Target e Remoção de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece82b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora, basta remover os outliers encontrados no target do dataset\n",
    "print(f\"Total de linhas antes de remover outliers: {X_train.shape[0]}\")\n",
    "X_train, y_train = remove_outliers(X_train, y_train)\n",
    "print(f\"Total de linhas depois de remover outliers: {X_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hist(y_train, 'target distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "c_log_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('log', FunctionTransformer(np.log1p, validate=False, feature_names_out='one-to-one')),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num', num_pipe, numerical),\n",
    "        ('clog', c_log_pipe, right_skewed),\n",
    "        ('cat', cat_pipe, categorical)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971eb4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessing_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113efe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestRegressor(\n",
    "    n_estimators=700,\n",
    "    max_leaf_nodes=16,\n",
    "    random_state=SEED,\n",
    "    n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebaeaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rnd_clf.feature_importances_\n",
    "\n",
    "feature_names = preprocessing_pipeline.get_feature_names_out()\n",
    "feature_importances_df = pd.DataFrame(zip(feature_names, importances), columns=['Feature', 'Importance']) \\\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importances_df = aggregate_categorical_importances(feature_importances_df.set_index('Feature'))\n",
    "\n",
    "top15_features = list(feature_importances_df[:14].index)+['cat__MS.Zoning']\n",
    "X_feats = [feat.split('__')[1] for feat in top15_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:, X_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7848752c",
   "metadata": {},
   "source": [
    "## Parte 3/4/5/6/..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8fa81",
   "metadata": {},
   "source": [
    "1. Escolher modelos (métodos de stacking inclusos)\n",
    "    - DummyRegressor\n",
    "    - LinearRegression\n",
    "    - Outros modelos básicos\n",
    "        - Polynomial Features\n",
    "        - Scalers\n",
    "    - Pipelines avançadas\n",
    "        - Utilizar KMeans como fonte de novas features na pipeline\n",
    "        - Métodos de Ensemble\n",
    "        \n",
    "2. Montar GridSearchCV com hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915eacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_skewed, numerical, categorical = split_by_prefix(top15_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d471821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) build the inner ColumnTransformer\n",
    "log_pipe = Pipeline([\n",
    "    (\"log1p\",   FunctionTransformer(np.log1p, validate=False)),\n",
    "    (\"impute\",  SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"impute\",  SimpleImputer(strategy=\"median\")),\n",
    "    # scaler will be overridden in grid\n",
    "    (\"scale\",   StandardScaler()),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"impute\",  SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
    "    (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\")),\n",
    "])\n",
    "\n",
    "base_preprocessor = ColumnTransformer([\n",
    "    (\"skewed\",   log_pipe,  right_skewed),\n",
    "    (\"numeric\",  num_pipe,  numerical),\n",
    "    (\"categorical\", cat_pipe, categorical),\n",
    "])\n",
    "\n",
    "kmeans_branch = Pipeline([\n",
    "    (\"pre\", base_preprocessor),\n",
    "    (\"cluster\", KMeans()),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) wrap in a FeatureUnion so we can add KMeans & Poly branches\n",
    "full_features = FeatureUnion([\n",
    "    (\"base\", base_preprocessor),\n",
    "    (\"kmeans\", kmeans_branch),\n",
    "    (\"poly\", Pipeline([\n",
    "        (\"select_num\", ColumnTransformer([\n",
    "            (\"num\", \"passthrough\", numerical)\n",
    "        ], remainder='drop')),\n",
    "        (\"poly\", PolynomialFeatures(include_bias=False)),\n",
    "    ])),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef456ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) single master pipeline\n",
    "pipe = Pipeline([\n",
    "    (\"features\",  full_features),\n",
    "    (\"regressor\", DummyRegressor()),  # placeholder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5596bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) custom RMSE scorer\n",
    "rmse = make_scorer(lambda y_true, y_pred: \n",
    "                   np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "                   greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b06fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) param_distributions as a list of dicts\n",
    "param_distributions = [\n",
    "\n",
    "    # ─────────── baseline regressors ───────────\n",
    "    {\n",
    "      \"regressor\": [DummyRegressor(), LinearRegression()],\n",
    "      \"features__kmeans__cluster__n_clusters\": [1, 2, 3, 4, 5, 6],\n",
    "      \"features__poly__poly__degree\": [1],   # no poly for baseline\n",
    "      \"features__base__numeric__scale\": [StandardScaler(), MinMaxScaler()],\n",
    "      # \"features__base__categorical__impute__strategy\": ['constant'],\n",
    "      # \"features__base__categorical__impute__fill_value\": ['MISSING'],\n",
    "    },\n",
    "\n",
    "    # ─────────── Ridge & Lasso ───────────\n",
    "    {\n",
    "      \"regressor\": [Lasso(), Ridge()],\n",
    "      \"regressor__alpha\": [0.1, 1, 10, 100],\n",
    "      \"features__kmeans__cluster__n_clusters\": [1, 2, 3, 4, 5, 6],\n",
    "      \"features__poly__poly__degree\": [1, 2],\n",
    "      \"features__base__numeric__scale\": [StandardScaler(), MinMaxScaler()],\n",
    "    },\n",
    "\n",
    "    # ────────── ElasticNet ──────────\n",
    "    {\n",
    "      \"regressor\": [ElasticNet()],\n",
    "      \"regressor__alpha\": [0.1, 1, 10, 100],\n",
    "      \"regressor__l1_ratio\": [0.1, 0.5, 0.9],\n",
    "      \"features__kmeans__cluster__n_clusters\": [1, 2, 3, 4, 5, 6],\n",
    "      \"features__poly__poly__degree\": [1, 2],\n",
    "      \"features__base__numeric__scale\": [None, StandardScaler(), MinMaxScaler()],\n",
    "    },\n",
    "\n",
    "    # ───────── RandomForest ─────────\n",
    "    {\n",
    "      \"regressor\": [RandomForestRegressor(random_state=42)],\n",
    "      \"regressor__n_estimators\": [500, 700, 1000],\n",
    "      \"regressor__max_depth\": [None, 10, 20, 30],\n",
    "      \"regressor__min_samples_split\": [2, 5, 10],\n",
    "      \"regressor__bootstrap\": [True, False],\n",
    "      \"features__kmeans__cluster__n_clusters\": [1, 2, 3, 4, 5, 6],\n",
    "      \"features__poly__poly__degree\": [1, 2, 3],\n",
    "      \"features__base__numeric__scale\": [None, StandardScaler(), MinMaxScaler()],\n",
    "    },\n",
    "\n",
    "    # ─────── GradientBoosting ───────\n",
    "    {\n",
    "      \"regressor\": [GradientBoostingRegressor(random_state=42)],\n",
    "      \"regressor__n_estimators\": [500, 700, 1000],\n",
    "      \"regressor__learning_rate\": [0.01, 0.05, 0.1],\n",
    "      \"regressor__max_depth\": [3, 5, 7],\n",
    "      \"regressor__subsample\": [0.6, 0.8, 1.0],\n",
    "      \"features__kmeans__cluster__n_clusters\": [1, 2, 3, 4, 5, 6],\n",
    "      \"features__poly__poly__degree\": [1, 2, 3],\n",
    "      \"features__base__numeric__scale\": [None, StandardScaler(), MinMaxScaler()],\n",
    "    },\n",
    "\n",
    "    # ─────────── XGBoost ───────────\n",
    "    {\n",
    "      \"regressor\": [xgb.XGBRegressor(random_state=42, objective=\"reg:squarederror\")],\n",
    "      \"regressor__n_estimators\": [500, 700, 1000],\n",
    "      \"regressor__learning_rate\": [0.01, 0.05, 0.1],\n",
    "      \"regressor__max_depth\": [3, 5, 7, 10],\n",
    "      \"regressor__subsample\": [0.6, 0.8, 1.0],\n",
    "      \"regressor__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "      \"regressor__reg_alpha\": [0, 0.1, 1, 10],\n",
    "      \"regressor__reg_lambda\": [1, 10, 100],\n",
    "      \"features__kmeans__cluster__n_clusters\": [1, 2, 3, 4, 5, 6],\n",
    "      \"features__poly__poly__degree\": [1, 2, 3],\n",
    "      \"features__base__numeric__scale\": [None, StandardScaler(), MinMaxScaler()],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) wrap in RandomizedSearchCV\n",
    "search = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,                    # sample 50 of these combos\n",
    "    scoring=rmse,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# 9) run it\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best RMSE:\", -search.best_score_)\n",
    "print(\"Best params:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855aaf9e",
   "metadata": {},
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0b90d",
   "metadata": {},
   "source": [
    "Seleção de modelos com GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315830cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    'regressor' : [LinearRegression(), DummyRegressor()],\n",
    "}, {\n",
    "    'regressor': [Lasso(), Ridge()],\n",
    "    'alpha': [0.1, 1, 10, 100],\n",
    "}, {\n",
    "    'regressor': [ElasticNet()],\n",
    "    'alpha': [0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0.1, 0.5, 0.9]\n",
    "}, {\n",
    "    'regressor': [RandomForestRegressor()],\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}, {\n",
    "    'regressor': [GradientBoostingRegressor()],\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}]\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=Pipeline(steps=[\n",
    "        ('preprocessor', preprocessing_pipeline),\n",
    "        ('regressor', RandomForestRegressor())\n",
    "    ]),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
